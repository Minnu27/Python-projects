# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TCA0nSFB0eB_iRfy8MH6Z5L7t_cC34u2
"""

! pip install opencv-python numpy torch torchvision deep_sort_realtime

! pip install yt-dlp

import yt_dlp

# List of YouTube video URLs
video_urls = [
    "https://www.youtube.com/watch?v=V9YDDpo9LWg",
    "https://www.youtube.com/watch?v=JBoc3w5EKfI",
    "https://www.youtube.com/watch?v=aWV7UUMddCU",
    "https://www.youtube.com/watch?v=f6wqlpG9rd0",
    "https://www.youtube.com/watch?v=GNVTuLHdeSo",
    "https://www.youtube.com/watch?v=SWtmkjd45so",
    "https://www.youtube.com/watch?v=RzI6Ar5mu2Q",
    "https://www.youtube.com/watch?v=aulLej6Z6W8",
    "https://www.youtube.com/watch?v=7pN6ydLE4EQ",
    "https://www.youtube.com/watch?v=fEEelCgBkWA",
    "https://www.youtube.com/watch?v=ckZQbQwM3oU",
    "https://www.youtube.com/watch?v=E8Wgwg3F4X0",
    "https://www.youtube.com/watch?v=rvIPH4ccfpI",
    "https://www.youtube.com/watch?v=F6iqlW6ovZc",
    "https://www.youtube.com/watch?v=9qjk-Sq415s&list=PL5B0D2D5B4BFE92C1&index=6",
    "https://www.youtube.com/watch?v=DI25kGJis0w",
    "https://www.youtube.com/watch?v=rrLhFZG6iQY",
    "https://www.youtube.com/watch?v=RKOZbT0ftL4&t=1s",
    "https://www.youtube.com/watch?v=N7TBbWHB01E",
    "https://www.youtube.com/watch?v=1YqVEVbXQ1c"
]

# Download each video
for url in video_urls:
    ydl_opts = {
        'format': 'bestvideo+bestaudio/best',
        'outtmpl': '%(title)s.%(ext)s',
        'noplaylist': True
    }
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        ydl.download([url])
#Extraction of frames for further use or analyses 
import cv2
import os

# Path to the video file
video_path = '/content/ABA Therapy - Learning about Animals.mp4'
# Directory where the frames will be saved
output_dir = '/content/output_frames'

# Create the output directory if it doesn't exist
os.makedirs(output_dir, exist_ok=True)

# Extract video name (without extension) for naming frames
video_name = os.path.splitext(os.path.basename(video_path))[0]

# Open the video file
cap = cv2.VideoCapture(video_path)
frame_count = 0

while True:
    # Read a frame
    ret, frame = cap.read()

    # If no frame is returned, we've reached the end of the video
    if not ret:
        break

    # Save the frame as an image
    frame_filename = f'{video_name}_frame_{frame_count:04d}.jpg'
    frame_filepath = os.path.join(output_dir, frame_filename)
    cv2.imwrite(frame_filepath, frame)

    frame_count += 1

# Release the video capture object
cap.release()

print("Frame extraction complete.")

import cv2
import torch
from deep_sort_realtime.deepsort_tracker import DeepSort
from google.colab.patches import cv2_imshow
# Load the detection model (e.g., YOLOv8)
model = torch.hub.load('ultralytics/yolov5', 'yolov5s')

# Initialize the tracker
tracker = DeepSort(max_age=30, n_init=3, nn_budget=100)

# Video input/output
cap = cv2.VideoCapture('/content/ABA Therapy - Learning about Animals.mp4')#give the path of the test videos
out = cv2.VideoWriter('output_video2.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 20, (int(cap.get(3)), int(cap.get(4))))

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Detect persons
    results = model(frame)
    detections = []
    for *box, conf, cls in results.xyxy[0].tolist():
        if cls in [0]:  # Assuming '0' is the class index for 'person'
            # Wrap bounding box and confidence in a list
            detections.append([[int(x) for x in box], conf])  # Corrected format

    # Update tracker
    tracked_objects = tracker.update_tracks(detections, frame=frame)

    # Overlay bounding boxes and IDs
    for track in tracked_objects:
        track_id = track.track_id
        ltrb = track.to_ltrb()
        # Convert coordinates to integers
        ltrb = [int(x) for x in ltrb]  # Convert coordinates to integers
        cv2.rectangle(frame, (ltrb[0], ltrb[1]), (ltrb[2], ltrb[3]), (0, 255, 0), 2)
        cv2.putText(frame, f"ID: {track_id}", (ltrb[0], ltrb[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

    out.write(frame)
    cv2_imshow(frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
out.release()
cv2.destroyAllWindows()